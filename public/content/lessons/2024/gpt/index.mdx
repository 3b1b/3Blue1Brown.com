---
title: But what is a GPT?  Visual intro to Transformers | Deep learning, chapter 5
description: A visual introduction to transformers. This chapter focusses on the overall structure, and word embeddings
date: 2024-04-01
video: wjZofJX0v4M
source: _2024/transformers/
credits:
- Lesson by Grant Sanderson
- Written Adaption by Justin Sun
---

## What is a GPT model?
Formally speaking, a GPT is a **Generative Pre-Trained Transformer**. The first two words are self-explanatory: **generative** means the model generates new text; **pre-trained** means the model was trained on large amounts of data. What we will focus on is the **transformer** aspect of the language model, the main proponent of the recent boom in AI.

### What exactly is a Transformer?
A **transformer** is a special kind of neural network, a _Machine Learning Model_. There are a wide variety of models that can be built using **transformers**: voice-to-text, text-to-voice, text-to-image, machine translation, and many more. The specific variant that we will focus on, which is the type that underlies tools like ChatGPT, will be a model trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, then produce a prediction of what comes next, in the form of a probability distribution over all chunks of text that might follow.

<Figure image="predict.png" width="100%"/>

At first, predicting the next word might feel like a different goal from generating new text, but once you have a prediction model like this, one simple way to make this generate a longer piece is to give it an initial bit of text to work with, have it predict the next word, take a random sample from the distribution it just generated, then run it all again to make a new prediction based on all the text including what it just added. This process of repeated prediction and sampling is essentially what’s happening when you interact with ChatGPT and see it producing one word at a time.

## What happens in a **Prediction Transformer**?
We'll first preview the transformer with a High-Level perspective. We’ll spend much more time motivating, interpreting, and expanding on the details of each step, but in broad strokes, when one of these chatbots is generating text, here’s what’s going on under the hood.


### Tokens
An input is first broken into small chunks that are known as **"tokens"**. For example, in the sentence:

**To date, the cleverest thinker of all time was ...**

The tokenization of this input would be:

**To | date | , | the | cle | ve | rest | think | er | of | all | time | was ...**

Each of these tokens is then associated with a vector, meaning some list of numbers. A common interpretation of these embeddings is that the coordinates of these vectors may somehow encode the meaning of each token. If you think of these vectors as giving coordinates in some high-dimensional space, words with similar meanings tend to land on vectors close to each other in that space. These steps are pre-processing steps that occur before anything enters the transformer itself.

<Figure image="token.png" width="100%"/>

### Attention Block
The encoded vectors then pass through an **Attention Block** where they communicate with each other to update their values based on context. For example, the meaning of the word "model" in the phrase “a machine learning model” is different from its meaning in the phrase “a fashion model”. The **Attention Block** is responsible for figuring out which words in the context are relevant to updating the meanings of other words, and how exactly those meanings should be updated.

<Figure image="multi.png" width="100%"/>

### Multilayer Perceptron(Feed-Forward Layer)
Following the Attention Block, these vectors then pass through a Multilayer Perceptron, or Feed-Forward Layer. Here, the vectors don’t talk to each other, they all go through the same operation in parallel. We’ll talk later about how this step is a bit like asking a long list of questions about each vector and updating them based on the answers.

<Figure image="attent.png" width="100%"/>

After passing through the Multilayer Perceptron, the vectors then return to the Attention Block and repeats the same process many times. Computationally, all the operations in both blocks will look like a giant pile of matrix multiplications, and our goal will be to understand how to read the underlying matrices. After many iterations, the eseential meaning of the passage has been somehow encoded into the very last vector in the sequence, which is then used to produce a probability distribution over all the possible chunks of text that might come next.

---

<Accordion title="Comprehension Question 1: What is a common interpretation of the coordinates of the vectors that are associated with the tokens?" children="A common interpretation is that the coordinates of these vectors may somehow encode the meaning of each token. If you think of these vectors as giving coordinates in some high-dimensional space, words with similar meanings tend to land on vectors close to each other in that space." />

---

With that as a high-level preview, in this lesson we will expand on the details of what happens both at the beginning and the end of the network. But first, we will dedicate a lot of time to reviewing important pieces of background knowledge that would have been second nature to any machine learning engineer by the time transformers came around.

## Premise of Deep Learning
Deep Learning is a subfield of Machine Learning, which describes any model where data is used to determine the model’s behavior. A function that takes in an image and produce a label describing it, or predicts the next word in a passage of text, or any other task that seems to require an element of pattern recognition and intuition are all examples of Machine Learning. Instead of explicitly defining a procedure for doing the desired task in code, a flexible structure with tunable parameters is set up, like a bunch of knobs and dials. Many examples of input-output pairs are then used to somehow tweak and tune these parameters to mimic the desired behavior.

For example, the simplest form of machine learning might be linear regression, where inputs and outputs are single numbers, such as the square footage of a house and its price. The goal is to find the line of best fit through the data to predict future house prices. This line is determined by two parameters: the slope and the y-intercept, which are tuned to most closely match the data.

<Figure image="linreg.png" width="100%"/>

Deep learning models are significantly more complex, building on foundational concepts to process vast amounts of data and recognize patterns. For example, GPT-3 has an astounding 175 billion parameters. However, designing such a massive model is no simple task—it faces challenges like severe overfitting or becoming completely intractable to train.

Despite these challenges, deep learning models have proven to scale remarkably well, thanks to a training algorithm all models use called backpropagation. In order for this algorithm to work at scale, these models have to adhere to a specific format. Understanding this format will help explain many of the choices in how transformers process language—choices that might otherwise seem arbitrary.

**First**, the input to the model has to be formatted as an array of real numbers. This could simply mean a list of numbers, a 2D array, or often higher dimensional arrays, where the general term used here is “tensor”. The input data is progressively transformed into many different layers, always structured like some array of numbers, until it reaches the final layer as the output. For example, the final layer in our model is the probability distribution over all possible next tokens.

In deep learning, the model parameters are almost always referred to as **weights**, because, and this is a key constraint for all your models, the only way they interact with the data being processed is through weighted sums. There are also some nonlinear functions sprinkled throughout, but they won’t depend on parameters. Typically, we would find the weighted sums packaged together as various components in a matrix-vector product. It represents the same idea, since each component in the output of a matrix-vector product looks like a weighted sum, it’s just often conceptually cleaner to think about matrices filled with tunable parameters transforming vectors drawn from the data being processed.

<Figure image="arrays.png" width="100%"/>

For example, the **175 billion weights** in GPT-3 are organized into just under 28,000 different matrices. Those matrices fall into 8 different categories, and we will step through each type to understand what it does. As we go through, it will be fun for us to reference the numbers from GPT-3 to count up exactly where those 175 billion parameters all come from. Our goal is to set the scene going inside a tool like ChatGPT when it generates text-almost all of the actual computation looks like matrix vector multiplication. There's a risk of getting lost in the vast amount of numbers, but a sharp distinction should be drawn between the weights of the model, colored in blue or red, and the data being processed, colored in grey. The weights are the actual brains of the model, learned during training and determining how it behaves. The data being processed encodes whatever specific input was fed into the model in a given instance, like an example snippet of text.

<Figure image="distinction.png" width="100%"/>

---

<Accordion title={
    <>
        Comprehension Question 2: The model parameters are commonly referred to as... <br /> 
        A. Tensors <br /> 
        B. Weights <br /> 
        C. Non-Linear Functions <br /> 
        D. None of the Above
    </>
} 
children={"The model parameters are almost always referred to as weights, as the only way models interact with the data being processed is through weighted sums."}
/>

---

## Embedding

With all that as a foundation, let’s dig into the text-processing example, starting with this first step of breaking up the input into little chunks, and turning those chunks into vectors. We mentioned earlier how these little chunks are called tokens, which might be pieces of words or punctuation. For the sake of simplicity, we will pretend that the input is clearly broken into words, as since we humans think in words, this will make it much easier to reference little examples to clarify each step.

### Embedding Matrix

The model has a predefined vocabulary, some list of all possible words, say 50,000 of them, and the first matrix of the transformer, known as the **embedding matrix**, will have one column for each of these words. These columns are what determines what vector each word turns into in that first step. We label it as $W_E$, and like all the matrices we see, its values begin random, but they're going to be learned based on data.

Turning words into vectors was common practice in machine learning long before transformers, and it sets the foundation for everything that follows, so let’s take a moment to get familiar with it. This is often called embedding the word, which invites thinking of these vectors geometrically as points or directions in some space. Visualizing a list of three numbers as coordinates for a point in 3D space is no problem, but word embeddings tend to be very high dimensional. For GPT-3, they have 12,288 dimensions, and as seen, it matters to work in a space with lots of distinct directions.

In the same way that you can take a 2D slice through 3D space, and project points onto that slice, for the sake of visualizing word embeddings, we will do something analogous by choosing a 3D _slice_ through the very high-dimensional space, projecting word vectors onto that, and displaying the result.

<Figure image="embedding.png" width="100%"/>

### Direction

The big idea we need to understand here is that as a model tweaks and tunes its weights to decide how exactly words get embedded as vectors during training, it tends to settle on a set of embeddings where directions in this space have semantic meaning. Below, a simple word-to-vector model is running, and when I run a search for all words whose embeddings are closest to that of “tower”, they all generally have the same vibe.

<Figure image="tower.png" width="100%"/>

Another classic example of this is when the difference between the vectors for _woman_ and _man_ is taken, which can be visualized as a vector in this space connecting the tip of one to the tip of the other. This difference is quite similar to the difference between _king_ and _queen_. So, if the word for a female monarch was unknown, it could be found by taking _king_, adding the direction of _woman_ minus _man_, and searching for the closest word embedding.

<Figure image="queen.png" width="100%"/>

At least, kind of. Despite this being the classic example, for this model the true embedding of _queen_ is a little farther off than the difference would suggest, presumably because the way _queen_ is used in training data is not merely a feminine version of a king. Family relations illustrate the idea better:

<Figure image="gender.png" width="100%"/>

The idea here is that it seems as if, during training, the model found it advantageous to choose embeddings such that one direction in this space encodes gender information.

Another example of this would be taking the embedding of _Italy_, subtracting the embedding of _Germany_, and adding it to the embedding of _Hitler_. This results in something very close to the embedding of _Mussolini_. It’s as if the model learned to associate some directions with Italian-ness and others with WWII Axis leaders.

<Figure image="italy.png" width="100%"/>

### Dot Product

One bit of mathematical intuition helpful to have in mind as we continue is how the dot product of two vectors can be thought of as measuring how well they align. Computationally, dot products involve multiplying all aligning components and adding the result. Geometrically, the dot product is positive when the vectors point in a similar direction, zero if they're perpendicular, and negative when they point in opposite directions. 

For example, suppose we wanted to test if the embedding of _cats_ minus that of _cat_ represents a kind of plurality direction in this space. To test this, notice what values we get when we take a dot product between this vector and various singular and plural nouns. It looks like the plural ones do indeed end up with consistently higher values than singular ones. Also, if we take this dot product with the embeddings of the words _one_, _two_, _three_ and so, they give increasing values, as if it’s quantitatively measuring how plural the model finds a given word.

<Figure image="dotProduct.png" width="100%"/>

Again, how specifically each word gets embedded is learned using data. The embedding matrix, whose columns store the embedding of each word, is the first pile of weights in our model. Using the GPT-3 numbers, the vocabulary size is 50,257, and again technically this consists not of words, per se, but different little chunks of text called tokens. The embedding dimension is 12,288, giving us **617,558,016 weights** in total for this first step. Let’s go ahead and add that to a running tally, remembering that by the end we should count up to 175 billion weights.

<Accordion title={
    <>
        Comprehension Question 3: In a vector space representing meanings, the following relationship holds: <br />
        <u> Cat - Meow + Bark = ? </u> <br />
        A. Tree <br /> 
        B. Sound <br /> 
        C. Dog <br /> 
        D. None of the Above
    </>
} 
children={
    "In this vector space, Cat - Meow represents a sort of sound direction. The animal that most commonly makes the bark sound is C. Dog"
    }
/>